{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EYHF_GR2iA80",
        "outputId": "7a5d5b23-ee8a-409b-c78c-60109f9a9174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
            "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
            " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
            " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
            " ...\n",
            " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
            " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
            " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data=load_breast_cancer()\n",
        "\n",
        "print(data.target)\n",
        "print(data.data)\n",
        "y = data.target\n",
        "X = data.data\n",
        "#these are numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkBGQe-SjhjA",
        "outputId": "66c30145-da1e-4849-ace7-31668d9b3fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#had done X=df.drop(['target'], axis =1)\n",
        "print(df.head())\n",
        "X = df # X is already defined in cell EYHF_GR2iA80 and 'target' column is already dropped."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKu3sL5RjhlV",
        "outputId": "03785cff-3198-46fa-c2b4-85620db9f845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
            "0                 0.07871  ...         25.38          17.33           184.60   \n",
            "1                 0.05667  ...         24.99          23.41           158.80   \n",
            "2                 0.05999  ...         23.57          25.53           152.50   \n",
            "3                 0.09744  ...         14.91          26.50            98.87   \n",
            "4                 0.05883  ...         22.54          16.67           152.20   \n",
            "\n",
            "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
            "0      2019.0            0.1622             0.6656           0.7119   \n",
            "1      1956.0            0.1238             0.1866           0.2416   \n",
            "2      1709.0            0.1444             0.4245           0.4504   \n",
            "3       567.7            0.2098             0.8663           0.6869   \n",
            "4      1575.0            0.1374             0.2050           0.4000   \n",
            "\n",
            "   worst concave points  worst symmetry  worst fractal dimension  \n",
            "0                0.2654          0.4601                  0.11890  \n",
            "1                0.1860          0.2750                  0.08902  \n",
            "2                0.2430          0.3613                  0.08758  \n",
            "3                0.2575          0.6638                  0.17300  \n",
            "4                0.1625          0.2364                  0.07678  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.columns)\n",
        "#y = df['target'] #this is still saved as series\n",
        "#now both X and y are df and series respectively\n",
        "#print(y.head()) #numpy so cant use it\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22, stratify= y) #theyre still df and series\n",
        "#stratify  both training and test respect original proportion of classes say 70% + and 30% - in total, either test or trained could be skewed s.t one has 90-10\n",
        "\n",
        "'''\n",
        "scaler=StandardScaler() #new isntance of fxn\n",
        "scaler.fit(X_train) # computes mean and std of each features\n",
        "X_train_scaled=scaler.transform(X_train) #applies it to x which is now numpy array with mean 0 and std 1\n",
        "X_test_scaled=scaler.transform(X_test) #same mean and std from training data, avoids-data leakage w/e thta is\n",
        "\n",
        "#y_train = y_train.to_numpy()\n",
        "#y_test = y_test.to_numpy()\n",
        "#not necessarily requried 'cause apparently scikit learn does it, but i prefer this\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "eUWo5GjIr6_x",
        "outputId": "0f5edfe4-9b26-489e-b609-7f4a9cfea22d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
            "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
            "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
            "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
            "       'smoothness error', 'compactness error', 'concavity error',\n",
            "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
            "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
            "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
            "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nscaler=StandardScaler() #new isntance of fxn\\nscaler.fit(X_train) # computes mean and std of each features\\nX_train_scaled=scaler.transform(X_train) #applies it to x which is now numpy array with mean 0 and std 1\\nX_test_scaled=scaler.transform(X_test) #same mean and std from training data, avoids-data leakage w/e thta is\\n\\n#y_train = y_train.to_numpy()\\n#y_test = y_test.to_numpy()\\n#not necessarily requried 'cause apparently scikit learn does it, but i prefer this\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7rRwIwC-rXhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REVIEW FROM HERE"
      ],
      "metadata": {
        "id": "2Jtbke0PrXqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.columns) #see all the features, going to manually code decision tree-> 30 features, this is dataframe->havent run standard scaler?\n",
        "\n",
        "# BEFORE SCALING\n",
        "feature_names = X_train.columns.tolist() #X_train is df, .columns outputs an object called Index which holds column names, .tolist() turns ojbect into python list of str\n",
        "\n",
        "# APPLY SCALING\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_np = scaler.fit_transform(X_train) #fit_transform does both computes mean and std of X_train and saves, then transform: applies on X_train\n",
        "X_test_scaled = scaler.transform(X_test) #scale based on X_train mean and std\n",
        "print(feature_names)\n",
        "\n",
        "# Ensure X_train and y_train are pandas with aligned indices\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled_np, columns=X_train.columns, index = X_train.index)  # Use original column names\n",
        "y_train = pd.Series(y_train, index=X_train.index)   # Align indices!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NVHEqaL-_sH",
        "outputId": "e1caeaf8-f68f-45b4-ae70-3481d022bbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
            "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
            "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
            "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
            "       'smoothness error', 'compactness error', 'concavity error',\n",
            "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
            "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
            "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
            "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
            "      dtype='object')\n",
            "['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree is like a sqeuence of questions: in this case a node is a feature with an asked q about threshold that splits data.\n",
        "#We want to find best feature first=root node then threshold to split\n",
        "#Gini gives us a way to define best split, need to measure how pure before can define best split"
      ],
      "metadata": {
        "id": "_MySFr0C-_z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(y): #define a fxn that takes y as input\n",
        "  #computes impurity for a set of binary class labels, here y = 1D  binary arary\n",
        "  if len(y) == 0:\n",
        "    return 0 #this is an edge case, good practice to code, if y is empty for some reason impurity would be undefined but we return 0 here instead\n",
        "    #get rid of indent so this is else when len(y) !==0\n",
        "  p0 = np.mean(y) #y is an array of 1 and 0s, so the mean is totalsum/total gives us a probability of # w/ class label 1 [proportion]\n",
        "  p1 = 1 - p0 #proportion of class label 0\n",
        "  return 1 - p1**2 - p0**2 #Gini formula for impurity measures how mixed, ideal is 100% predictor of a single class 0=pure and .5 max impure"
      ],
      "metadata": {
        "id": "URZ0wONbEhTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, y, feature, threshold): #returns x_left, y_left, etc.. in order by default\n",
        "    y = pd.Series(y.values, index=X.index)  # align y's index to X index\n",
        "    left_mask = X[feature] <= threshold #creates Boolean mask labelling T or F in rows of X\n",
        "    right_mask = X[feature] > threshold\n",
        "\n",
        "    return X[left_mask], y[left_mask], X[right_mask], y[right_mask] #class labels correspond (Have to include!!)"
      ],
      "metadata": {
        "id": "tPVBKsjQK9jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#e.g.\n",
        "feature = 'mean radius'\n",
        "threshold = 14.0\n",
        "\n",
        "X_left, y_left, X_right, y_right = split_dataset(X_train_scaled, y_train, feature, threshold)\n",
        "\n",
        "print(\"Left size:\", len(y_left), \"| Gini:\", gini_impurity(y_left))\n",
        "print(\"Right size:\", len(y_right), \"| Gini:\", gini_impurity(y_right))\n",
        "\n"
      ],
      "metadata": {
        "id": "hGUtQ_NGGBhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc94e07-6f2c-4666-c922-6c7c0ba582e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left size: 455 | Gini: 0.4680594131143581\n",
            "Right size: 0 | Gini: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted gini is nL/n * gini(L) + nR/n *gini(R) this is overall impurity and needs to be minimized\n",
        "#for each feature e.g. mean radius, want to find each unique value  in that feature=possible threshold,\n",
        "#calculate (weighted) gini of every value of every feature to then get best split\n",
        "#gini is for a node, weighted gini is for a split\n",
        "#in e.g. we have gini for child node left and child node right separately, weighted gini has a computation of 0.22- so of the entire split"
      ],
      "metadata": {
        "id": "k-JjcL8qNCpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "528FV8krQmnt",
        "outputId": "0f393283-1a37-497e-b7ce-16e42c7d5fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n",
            "569 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split(X, y):\n",
        "    best_feature = None             # To store the feature that gives the best split\n",
        "    best_threshold = None           # To store the threshold value for that split\n",
        "    best_gini = float('inf')        # Initialize best Gini to infinity (we want to minimize it)\n",
        "    best_splits = None\n",
        "\n",
        "    n_samples, n_features = X.shape # Number of samples and features unpacked from X.shape ito 2 variables - good for looping\n",
        "\n",
        "    # Loop over every feature (column) in X\n",
        "    for feature_idx in range(n_features): #looping over feature indices-dont yet have a name\n",
        "        feature_name = X.columns[feature_idx]    # X.column is list of names and we're assinging indices to actual names (assuming X is a DataFrame)\n",
        "        values = X[feature_name].unique()        # Get all unique values in this feature as possible thresholds\n",
        "\n",
        "        # Loop over every unique threshold value for this feature\n",
        "        for threshold in values:\n",
        "            # Split dataset based on current feature and threshold\n",
        "            X_left, y_left, X_right, y_right = split_dataset(X, y, feature_name, threshold)\n",
        "\n",
        "            # Calculate Gini impurity for left and right splits\n",
        "            gini_left = gini_impurity(y_left)\n",
        "            gini_right = gini_impurity(y_right)\n",
        "\n",
        "            # Calculate weighted average of impurities\n",
        "            n_left = len(y_left)\n",
        "            n_right = len(y_right)\n",
        "            weighted_gini = (n_left / n_samples) * gini_left + (n_right / n_samples) * gini_right\n",
        "\n",
        "            # Update best split if current split is better (lower weighted Gini)\n",
        "            if weighted_gini < best_gini:\n",
        "                best_gini = weighted_gini\n",
        "                best_feature = feature_name\n",
        "                best_threshold = threshold\n",
        "                best_splits = (X_left, y_left, X_right, y_right)\n",
        "\n",
        "    # Return the best feature, threshold and the corresponding Gini impurity\n",
        "    return best_feature, best_threshold, best_splits\n"
      ],
      "metadata": {
        "id": "7MDf9P44NCrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_feature, best_threshold, (X_left, y_left, X_right, y_right) = best_split(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best Feature to Split On:\", best_feature)\n",
        "print(\"Best Threshold for Split:\", best_threshold)\n",
        "print(\"Left Split Size:\", len(X_left), \"| Gini:\", gini_impurity(y_left))\n",
        "print(\"Right Split Size:\", len(X_right), \"| Gini:\", gini_impurity(y_right))\n",
        "#This is the ROOT NODE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BWZ3VyiUczy",
        "outputId": "b4b9bbf8-8f7f-4898-d3fa-81628a41541a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Feature to Split On: worst area\n",
            "Best Threshold for Split: -0.016532311084756245\n",
            "Left Split Size: 306 | Gini: 0.15549574949805645\n",
            "Right Split Size: 149 | Gini: 0.0648619431557136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building a full binary tree, not just splitting once: each internal node is a decison like mean > x or a leaf node which holds a prediction\n",
        "#need to store node's info: feature & threshhold, its children left & right and identify whether internal or leaf node\n",
        "#traverse later to make a prediction, so tree node is a container for information"
      ],
      "metadata": {
        "id": "dEAwD0OUmv-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeNode:\n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, *, value =None): #construct that defines what it holds, * is a trick so value is passed as keyword=prediction\n",
        "        self.feature = feature #stores input arguments as instance variables\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value #class label if leaf;\n",
        "  def is_leaf(self):  #if value is not None then it's a leaf node\n",
        "        return self.value is not None"
      ],
      "metadata": {
        "id": "wZut800wmwAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recursively iterate after root node on both left and right subsets indepedently. Can reuse the feature that is the root node with a diff threshold\n",
        "#if a feature never provides a best split it may not be used as a node"
      ],
      "metadata": {
        "id": "tuHjemLSxKmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree(X, y, depth = 0, max_depth = 5): #depth=0 is start ar root node and max depth is where we stop so I guess here a total of 6 nodes possible\n",
        "  # Convert y to pandas Series if it's a numpy array\n",
        "  if isinstance(y, np.ndarray):\n",
        "    y = pd.Series(y)\n",
        "\n",
        "  num_samples_per_class = y.value_counts().to_dict() #value_counts() tells us how many 1s and 0s in y e.g. 4 1s and 2 0s, to_dict then makes it a dictionarty {1:4, 0:2}\n",
        "  predicted_class = max(num_samples_per_class, key= num_samples_per_class.get) #max() goes through each key and applies .get() to them, returning a count, key=label(0 or 1)\n",
        "#num.get(1) will return 4, and get(0)  returns 2, then we pick the key with the max count\n",
        "  #stopping condition\n",
        "  if len(num_samples_per_class) == 1 or depth >= max_depth:\n",
        "  #len() how many different labels present in y b/c its lenght of num_samples which is a dictionary if {1:4} its length is 1 if {1:4,0:2} len=2\n",
        "    return TreeNode(value = predicted_class) #calls fxn tree node and b/c value not None, it's a leaf node otherwsie it has features, thresholds, left, right\n",
        "\n",
        "  #find best split\n",
        "  feature, threshold, splits = best_split(X,y) #applies loop that we initially used to find the root node which outputs feature, threshold and splits\n",
        "  if splits is None:\n",
        "    return TreeNode(value = predicted_class) #no split found/possible then just call this a leaf node\n",
        "\n",
        "  #Recusively build left and right trees\n",
        "  X_left, y_left, X_right, y_right = splits\n",
        "\n",
        "  left_subtree = build_tree(X_left, y_left, depth + 1, max_depth)\n",
        "  right_subtree = build_tree(X_right, y_right, depth + 1, max_depth)\n",
        " # 4. Return current node with children\n",
        "  return TreeNode(feature=feature, threshold=threshold, left=left_subtree, right=right_subtree) #returns object with feature, threshold and left & right which are also TreeNodes"
      ],
      "metadata": {
        "id": "WpqyPmpc0mc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actually build the tree (training phase)\n",
        "tree = build_tree(X_train_scaled, y_train, max_depth=5)\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_EsiTx90mfK",
        "outputId": "67054eb9-08c2-432d-e9ca-4fb3242eb5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.TreeNode object at 0x7d75887fa090>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tree(node, depth=0):\n",
        "    indent = \"  \" * depth\n",
        "    if node.value is not None:  # Leaf node\n",
        "        print(f\"{indent}Predict: {node.value}\")\n",
        "    else:\n",
        "        print(f\"{indent}Feature: {node.feature} <= {node.threshold:.3f}?\")\n",
        "        print(f\"{indent}--> True:\")\n",
        "        print_tree(node.left, depth + 1)\n",
        "        print(f\"{indent}--> False:\")\n",
        "        print_tree(node.right, depth + 1)\n"
      ],
      "metadata": {
        "id": "dA1JnFYEAHk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_tree(tree)\n",
        "\n"
      ],
      "metadata": {
        "id": "95fjXRwPAKjW",
        "outputId": "1cbea8e4-4660-4d6f-f217-5570f59e1c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature: worst area <= -0.017?\n",
            "--> True:\n",
            "  Feature: worst concave points <= 0.418?\n",
            "  --> True:\n",
            "    Feature: worst perimeter <= -0.007?\n",
            "    --> True:\n",
            "      Feature: radius error <= 0.819?\n",
            "      --> True:\n",
            "        Feature: smoothness error <= -1.316?\n",
            "        --> True:\n",
            "          Predict: 1\n",
            "        --> False:\n",
            "          Predict: 1\n",
            "      --> False:\n",
            "        Feature: mean radius <= -0.672?\n",
            "        --> True:\n",
            "          Predict: 0\n",
            "        --> False:\n",
            "          Predict: 1\n",
            "    --> False:\n",
            "      Feature: mean texture <= 0.282?\n",
            "      --> True:\n",
            "        Feature: worst smoothness <= -0.040?\n",
            "        --> True:\n",
            "          Predict: 1\n",
            "        --> False:\n",
            "          Predict: 0\n",
            "      --> False:\n",
            "        Predict: 0\n",
            "  --> False:\n",
            "    Feature: worst symmetry <= -0.324?\n",
            "    --> True:\n",
            "      Feature: mean texture <= 0.210?\n",
            "      --> True:\n",
            "        Predict: 1\n",
            "      --> False:\n",
            "        Predict: 0\n",
            "    --> False:\n",
            "      Feature: mean radius <= 0.206?\n",
            "      --> True:\n",
            "        Predict: 0\n",
            "      --> False:\n",
            "        Predict: 1\n",
            "--> False:\n",
            "  Feature: worst concavity <= -0.184?\n",
            "  --> True:\n",
            "    Feature: mean texture <= 0.034?\n",
            "    --> True:\n",
            "      Predict: 1\n",
            "    --> False:\n",
            "      Predict: 0\n",
            "  --> False:\n",
            "    Predict: 0\n"
          ]
        }
      ]
    }
  ]
}