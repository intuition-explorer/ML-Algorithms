{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9zQHwgpHrNEuvhYNYPv4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intuition-explorer/ML-Algorithms/blob/main/PalmerPenguins_KNN_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GdyrpBZXiDX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold #Kfold is new to me i want to try\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder #OneHotEncoder might need\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#NEW to me now\n",
        "from sklearn.datasets import fetch_openml #can get dataset from openml based on name\n",
        "from sklearn.pipeline import Pipeline #simplifies the process of applying multisteps: imputation, scaling, and model training, by treating them as a single object\n",
        "from sklearn.compose import ColumnTransformer #good for heterogenous data: numerical may need scaling and categorical one hot encoding, applies to specific col\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = sns.load_dataset(\"penguins\") #this is a dataframe already\n",
        "X = data.drop('species', axis=1).copy()\n",
        "y=data['species'].copy()\n",
        "X.shape #344 samples, 6 features, y.shape matches features\n",
        "X.info() #missing features for 2 penguins body and 11 missing sex\n",
        "#Learnt not to replace data with 0, I'm thinking mean or median but really depends on the type of penguin...\n",
        "\n",
        "#IMPUTATION-assignment of a value to something based on inference of related data, can try using the pipeline approach\n",
        "\n",
        "y=y.to_numpy() #Just prudent to do so for later\n",
        "y=pd.Series(y)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSDBt1A9Z3EM",
        "outputId": "14de39b3-30b5-4d7e-f61f-0dc24daf9dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 344 entries, 0 to 343\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   island             344 non-null    object \n",
            " 1   bill_length_mm     342 non-null    float64\n",
            " 2   bill_depth_mm      342 non-null    float64\n",
            " 3   flipper_length_mm  342 non-null    float64\n",
            " 4   body_mass_g        342 non-null    float64\n",
            " 5   sex                333 non-null    object \n",
            "dtypes: float64(4), object(2)\n",
            "memory usage: 16.3+ KB\n",
            "0      Adelie\n",
            "1      Adelie\n",
            "2      Adelie\n",
            "3      Adelie\n",
            "4      Adelie\n",
            "        ...  \n",
            "339    Gentoo\n",
            "340    Gentoo\n",
            "341    Gentoo\n",
            "342    Gentoo\n",
            "343    Gentoo\n",
            "Length: 344, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to learn how to identify outliers\n",
        "stats = X.describe()\n",
        "print(stats)\n",
        "stats = stats.T\n",
        "#COMPARE MIN AND MAX TO Q1 AND Q3\n",
        "type(stats)\n",
        "stats.head()\n",
        "\n",
        "Q1 = stats['25%']\n",
        "Q3 = stats['75%']\n",
        "IQR = Q3 - Q1\n",
        "l_bound = Q1 - 1.5*IQR\n",
        "u_bound = Q3 + 1.5*IQR\n",
        "\n",
        "print('\\n', l_bound)\n",
        "print('\\n', u_bound)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1Rl4XEei0L6",
        "outputId": "8e58fe25-bc3d-4b52-ea7f-2e2fa572b0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n",
            "count      342.000000     342.000000         342.000000   342.000000\n",
            "mean        43.921930      17.151170         200.915205  4201.754386\n",
            "std          5.459584       1.974793          14.061714   801.954536\n",
            "min         32.100000      13.100000         172.000000  2700.000000\n",
            "25%         39.225000      15.600000         190.000000  3550.000000\n",
            "50%         44.450000      17.300000         197.000000  4050.000000\n",
            "75%         48.500000      18.700000         213.000000  4750.000000\n",
            "max         59.600000      21.500000         231.000000  6300.000000\n",
            "\n",
            " bill_length_mm         25.3125\n",
            "bill_depth_mm          10.9500\n",
            "flipper_length_mm     155.5000\n",
            "body_mass_g          1750.0000\n",
            "dtype: float64\n",
            "\n",
            " bill_length_mm         62.4125\n",
            "bill_depth_mm          23.3500\n",
            "flipper_length_mm     247.5000\n",
            "body_mass_g          6550.0000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify outliers in numerical columns\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns\n",
        "outlier_condition = (X[numerical_cols] < l_bound) | (X[numerical_cols] > u_bound)\n",
        "\n",
        "# Select rows with outliers\n",
        "outliers = X[outlier_condition.any(axis=1)] #checks row wise if any column in that row is true\n",
        "print(outliers) #outputs the row, so no outliers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1JAtFHopzJe",
        "outputId": "1ab88474-3ead-4ed4-add9-558a9ff6280f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Can use this approach when have a huge dataset of features, good way to distinguish when I cant visually see\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist() #select only columns with numbers\n",
        "cat_cols = [c for c in X.columns if c not in num_cols] #pull out other columns\n",
        "print(num_cols)\n",
        "print(cat_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIHkrYOUa02y",
        "outputId": "e6a6aa0f-8ed4-4c62-e263-7c91fd61021d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
            "['island', 'sex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#so tedious to do it this way and have to remove and add columns, use pipeline and Column transformer instead\n",
        "manXfill = X[['bill_depth_mm', 'body_mass_g', 'bill_length_mm', 'flipper_length_mm']].fillna(X[['bill_depth_mm', 'body_mass_g', 'bill_length_mm', 'flipper_length_mm']].median())"
      ],
      "metadata": {
        "id": "qpCBq_xEhiEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(num_cols) #MUST be a list to put into column transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62cWs6cOa08L",
        "outputId": "552b86bf-2dd5-4b49-e73a-6f51e41192f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor1 = ColumnTransformer(transformers=[('num_imputer',SimpleImputer(strategy='median'), num_cols), #type of transform in str, then impute, then specified col\n",
        "                                                ('cat_imputer',SimpleImputer(strategy='most_frequent'), cat_cols)],\n",
        "                                remainder= 'passthrough') #other cols not affected, passthrough\n",
        "\n",
        "\n",
        "t_X1 = preprocessor1.fit_transform(X) #outputs numpy, should run on X_train as fit_transform and just .fit on X_val and X_test"
      ],
      "metadata": {
        "id": "Sib3tDiiuuFz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#With Pipeline as well can prevent leakage and does model tuning, CV, etc all in one step good for practice, wont forget training etc\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('num_imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), num_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "                                                            #unknown cat not present during fit is vec of all 0s, when new test not in training instead of error\n",
        "                                                            #sprase fals outputs numpy else scipy array which is more memory efficient for larger\n",
        "    ]), cat_cols)\n",
        "], remainder='drop') #eliminates any other types of col instead of passthrough"
      ],
      "metadata": {
        "id": "qWYwpLvDD5LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string labels into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_int = le.fit_transform(y)       # integer-encoded labels\n",
        "y_classes = le.classes_           # original class names in order\n",
        "\n",
        "print(y[:5])       # original labels (e.g. ['Adelie', 'Adelie', ...])\n",
        "print(y_int[:5])   # encoded labels (e.g. [0, 0, ...])\n",
        "print(y_classes)   # mapping (e.g. ['Adelie' 'Chinstrap' 'Gentoo'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V1Vuwmt_INor",
        "outputId": "87ea8aa6-1ba8-4f9d-c824-244d981b6c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    Adelie\n",
            "1    Adelie\n",
            "2    Adelie\n",
            "3    Adelie\n",
            "4    Adelie\n",
            "dtype: object\n",
            "[0 0 0 0 0]\n",
            "['Adelie' 'Chinstrap' 'Gentoo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "#This is good for big datasets-X_train,temp,val, tune hyperparameters on validation, but for smaller try StratifiedKFolds\n",
        "\n",
        "#To use both, ideal: separate test untouched set using above then use remaining for stratification-training and validation, where each rotates\n",
        "#X_temp, X_test, y_temp, y_test = train_test_split(X, y_int, test_size=0.2, random_state=42, stratify=y_int)\n",
        "'''\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# use the folds manually to pick one as validation, rest as train\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_temp,y_temp)):\n",
        "  X_train = X.iloc[train_idx]\n",
        "  y_train = y.iloc[train_idx] #if series then .iloc, else numpy none\n",
        "  X_val = X.iloc[val_idx]\n",
        "  y_val = y.iloc[val_idx]\n",
        "\n",
        "  # fit_transform on train\n",
        "  X_train_processed = preprocessor.fit_transform(X_train)\n",
        "  # transform on validation\n",
        "  X_val_processed = preprocessor.transform(X_val)\n",
        "  X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "  # Can also define model here, fit it to train then cross validate to get score\n",
        "  model = LogisticRegression(max_iter=1000)\n",
        "  model.fit(X_train_processed, y_train)\n",
        "  score = model.score(X_val_processed, y_val)\n",
        "  scores.append(score)\n",
        "  print(\"CV scores:\", scores)\n",
        "  print(\"Mean:\", np.mean(scores))\n",
        "\n",
        "\n",
        "  print(\"Train class distribution:\", y_train.value_counts().to_dict())\n",
        "  print(\"Val   class distribution:\", y_val.value_counts().to_dict())\n",
        "  print()\n",
        "'''\n",
        "#Score here outputs a metric depending on model type: classifieraccuracy, regressorcoefficientR, clusterNegativeInertia (sum os square dist to cluster center)"
      ],
      "metadata": {
        "id": "598SiAf1uuNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ec516759-fb8c-44ce-8935-75b48a7d3ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n# use the folds manually to pick one as validation, rest as train\\nfor fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_temp,y_temp)):\\n  X_train = X.iloc[train_idx]\\n  y_train = y.iloc[train_idx] #if series then .iloc, else numpy none\\n  X_val = X.iloc[val_idx]\\n  y_val = y.iloc[val_idx]\\n\\n  # fit_transform on train\\n  X_train_processed = preprocessor.fit_transform(X_train)\\n  # transform on validation\\n  X_val_processed = preprocessor.transform(X_val)\\n  X_test_processed = preprocessor.transform(X_test)\\n\\n  \\n  # Can also define model here, fit it to train then cross validate to get score\\n  model = LogisticRegression(max_iter=1000)\\n  model.fit(X_train_processed, y_train)\\n  score = model.score(X_val_processed, y_val)\\n  scores.append(score)\\n  print(\"CV scores:\", scores)\\n  print(\"Mean:\", np.mean(scores))\\n  \\n\\n  print(\"Train class distribution:\", y_train.value_counts().to_dict())\\n  print(\"Val   class distribution:\", y_val.value_counts().to_dict())\\n  print()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "  # transform on validation\n",
        "X_test_processed = preprocessor.transform(X_temp)\n"
      ],
      "metadata": {
        "id": "k_WfuQ0Mz_oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#Cleaner way with pipeline\n",
        "pipe = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_temp, y_temp):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    pipe.fit(X_train, y_train)       # pipeline fits preprocessor + model\n",
        "    score = pipe.score(X_val, y_val) # pipeline transforms + predicts\n",
        "    print(\"Fold score:\", score)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "RIJO2DW-OyT_",
        "outputId": "a894b6d8-9e7e-4e27-e960-3b74ed65c2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LogisticRegression' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1979295288.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m pipe = Pipeline(steps=[\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "KNC = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "KNC.fit(X_train_processed, y_train)\n",
        "y_pred = KNC.predict(X_test_processed)\n",
        "\n",
        "# Convert predicted labels to integers using the fitted LabelEncoder\n",
        "y_pred_int = le.transform(y_pred)\n",
        "\n",
        "print(classification_report(y_temp, y_pred_int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD8vsaPivztM",
        "outputId": "75c31ba2-1f24-4e05-c579-ca0b970511ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        30\n",
            "           1       0.93      1.00      0.97        14\n",
            "           2       1.00      1.00      1.00        25\n",
            "\n",
            "    accuracy                           0.99        69\n",
            "   macro avg       0.98      0.99      0.98        69\n",
            "weighted avg       0.99      0.99      0.99        69\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN metric, this is scalar only..\n",
        "def pairwise_distances(A, B, metric=\"euclidean\"):\n",
        "    A=np.array(A)\n",
        "    B=np.array(B)\n",
        "    if metric == 'euclidean':\n",
        "      return np.sqrt((A-B)**2)\n",
        "    if metric == 'manhattan':\n",
        "      return np.sum(np.abs(A-B))\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported metric\")\n",
        "\n",
        "print(pairwise_distances([1,2],[2,4], 'manhattan'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfoydaE4OOEE",
        "outputId": "85b52308-171b-43b0-cb31-5da089d1c803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN metric vectorized\n",
        "#A= a_n,d B=b_n,d\n",
        "def pairwise_distances(A, B, metric=\"euclidean\"):\n",
        " if metric == 'euclidean':\n",
        "  A2 = np.sum(A**2, axis =1, keepdims=True) #squares all vals in the array, then sums them up by the column, then keepsdim: shape n_a,1 instead of n_a\n",
        "  B2 = np.sum(B**2, axis =1)[None, :] #gets shape n_b, with None,:, then add a column infront to get 1,n_b #good for BROADCASTING\n",
        "#None adds new axis, here array is a tensor axis=0 row axis=1 column, and : is a slice operator-keeps all the features\n",
        "#[None,:] goes from n_b to 1, n_b\n",
        "\n",
        "  AB = A @ B.T #Multiply vectors/arrays instead of scalar approach i tried above and shapes work b/c of steps above\n",
        "  D2 = A2 + B2 - 2*AB\n",
        "  D2[D2<0] = 0 #safety due to floating points so get no small negatives,\n",
        "  return np.sqrt(D2)\n",
        " if metric == 'manhattan':\n",
        "  np.sum(np.abs(A[:, None, :] - B[None, :, :]), axis=2) #A becomes (n_a, 1, d) B: (1, n_b, d)\n",
        "  #A-B yields (n_a, n_b, d), axis =2 says to sum up only 3rd axis here\n",
        " else:\n",
        "  raise ValueError(\"Unsupported metric\")\n"
      ],
      "metadata": {
        "id": "s9KmxpTPONdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN- majority vote\n",
        "# (distance_weighted = False): didnt include if, distance weighted false is take avg or majority, if true closer values matter more\n",
        "\n",
        "class KNN:\n",
        "  def __init__(self, k=1, metric = 'euclidean'):\n",
        "    self.k = k #self is the instance of the object, allows object to store its own data & parameters\n",
        "    self.metric = metric\n",
        "   # self.distance_weighted = distance_weighted\n",
        "    self.X_ = None #place holders for data later, when define fit store them\n",
        "    self.y_ = None\n",
        "    self.classes_ = None\n",
        "\n",
        "  def fit(self, X, y): #later we will do model.fit or I guess KNN.fit\n",
        "    self.X_ = np.array(X)\n",
        "    self.y_ = np.array(y_int)\n",
        "    self.classes_ = np.unique(self.y_)\n",
        "    return self #stores everything in its memory above so can do method chaining\n",
        "#So far just memorized training set, stores classes and hyperparameters\n",
        "\n",
        "  def predict(self, X):\n",
        "    X = np.array(X) #new test points you wan to compare, makes np array from data frame\n",
        "    D = pairwise_distances(X, self.X_, metric=self.metric) #[n_test, n_train] computes distance between all test and training points\n",
        "\n",
        "#D.shape thus has rows=test points & columns=training points (from dimensions of X_test, X_train)\n",
        "#if 2 by 2 test and 3 x 2 training, then D is 2 x 3 matrix full of Euclidean distances\n",
        "\n",
        "    #Dont need for vectorized :y_pred = []\n",
        "\n",
        "#D.shape[0] then is number of rows=test points (distances from diff targets on columns)\n",
        "\n",
        "    for i in range(D.shape[0]): #loop over each test point, e.g. i[0] is test (1,2) i[1] test (2,3)\n",
        "      #neighbors_idx = np.argsort(D[i])[:k] #this gets indices of sorted distances arranged from smallest to highest, the [:k] then takes first k indices\n",
        "\n",
        "      #Vectorize above, faster and does all rows at once instead of one at a time\n",
        "      neighbors_idx = np.argsort(D, axis=1)[:, :self.k]  # shape [n_test, k]\n",
        "#axis=1 is row for a test point now across all training points, this outputs an array of indicies of training points (to test point) in order\n",
        "# the : is same as 0:end, so takes all rows then :self.k takes columns from 0 to k-1\n",
        "\n",
        "      neighbor_label = self.y_[neighbors_idx]#from the indices of these points(training) get the corresponding y_label\n",
        "      #This is now shape [n_test, k]\n",
        "\n",
        "     # pred = np.bincount(neighbor_label).argmax() #count labels if [0,1] then yields [1,1] (1 for each 0 and 1) then arg max, picks first max: here 0\n",
        "      #if bincount of label [1,1] then yields 0 0s and 2 1s=[0,2] then picks max 2: so label is 1\n",
        "\n",
        "      #vectorize above now\n",
        "      y_pred = np.array([np.bincount(row, minlength=len(self.classes_)).argmax() for row in neighbor_label])\n",
        "#for row in n_label: loop over test point's neighbours, each row is k,\n",
        "#min length counts number of classes (works for more than 2), here just 2, so bincount here counts # of classes occurring in a row\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "MD_utisHlwRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "#THIS WAS A FIX PUTTING LABEL ENCODER IN THE LOOP\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=1, metric=\"euclidean\"):\n",
        "        self.k = k\n",
        "        self.metric = metric\n",
        "        self.X_ = None\n",
        "        self.y_ = None\n",
        "        self.classes_ = None\n",
        "        self.le = None  # will hold the encoder\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_ = np.array(X)\n",
        "        self.le = LabelEncoder()\n",
        "        self.y_ = self.le.fit_transform(y)  # now guaranteed 0..n-1\n",
        "        self.classes_ = self.le.classes_\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        D = pairwise_distances(X, self.X_, metric=self.metric)\n",
        "        neighbors_idx = np.argsort(D, axis=1)[:, :self.k]\n",
        "        neighbor_label = self.y_[neighbors_idx]   # now ints 0..n-1\n",
        "        y_pred_int = np.array([\n",
        "            np.bincount(row, minlength=len(self.classes_)).argmax()\n",
        "            for row in neighbor_label\n",
        "        ])\n",
        "        return self.le.inverse_transform(y_pred_int)\n"
      ],
      "metadata": {
        "id": "DtcTnjmty3-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = KNN()\n",
        "clf.fit(X_train_processed, y_train)               # y_train = strings\n",
        "y_test_pred = clf.predict(X_test_processed)       # y_test_pred = strings\n",
        "print(confusion_matrix(y_temp, y_test_pred))\n",
        "print(classification_report(y_temp, y_test_pred))\n",
        "\n",
        "\n",
        "#IT WORKS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# I HAD ISSUE WITH Y BEING STRINGS VS INTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bht6Drc1zYVM",
        "outputId": "968d5914-e36a-437d-8151-cdb8542bebda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[29  1  0]\n",
            " [ 0 14  0]\n",
            " [ 0  0 25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      0.97      0.98        30\n",
            "   Chinstrap       0.93      1.00      0.97        14\n",
            "      Gentoo       1.00      1.00      1.00        25\n",
            "\n",
            "    accuracy                           0.99        69\n",
            "   macro avg       0.98      0.99      0.98        69\n",
            "weighted avg       0.99      0.99      0.99        69\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifier\n",
        "clf = KNN()\n",
        "# Fit on **processed training data**\n",
        "clf.fit(X_train_processed, y_train)\n",
        "\n",
        "#predict on\n",
        "y_test_pred = clf.predict(X_test_processed)\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print(np.unique(y_int))\n",
        "\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "#y_test_pred_int = le.transform(y_test_pred)\n",
        "\n",
        "#print(classification_report(y_test, y_test_pred_int))\n",
        "#okay works\n",
        "'''\n",
        "           0       0.79      1.00      0.88        30\n",
        "           1       0.19      0.43      0.27        14\n",
        "           2       0.00      0.00      0.00        25\n",
        "\n",
        "    accuracy                           0.52        69\n",
        "   macro avg       0.33      0.48      0.38        69\n",
        "weighted avg       0.38      0.52      0.44        69\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hYHE-R8S7Bs5",
        "outputId": "5afd76e3-c8d0-4370-be4a-4348151631c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mix of label input types (string and number)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-324246572.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m         \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KNC.fit(X_train_processed, y_train)\n",
        "y_pred = KNC.predict(X_test_processed)\n",
        "\n",
        "# Convert predicted labels to integers using the fitted LabelEncoder\n",
        "y_pred_int = le.transform(y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred_int))\n",
        "#Same as chat GPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twchl30kwZV9",
        "outputId": "f8e37324-b85a-4345-90dd-0b7fb01c9b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        30\n",
            "           1       0.93      1.00      0.97        14\n",
            "           2       1.00      1.00      1.00        25\n",
            "\n",
            "    accuracy                           0.99        69\n",
            "   macro avg       0.98      0.99      0.98        69\n",
            "weighted avg       0.99      0.99      0.99        69\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STUDY THEIR PERFECT OUTPUT:"
      ],
      "metadata": {
        "id": "ebciRVW5mTGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Repro + Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "# 1) Load data (Palmer Penguins)\n",
        "# Option A: seaborn.load_dataset('penguins') if you prefer.\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset(\"penguins\").dropna(subset=[\"species\"])  # keep rows with species label\n",
        "\n",
        "# 2) Define target + features\n",
        "y = df[\"species\"].to_numpy()\n",
        "X = df.drop(columns=[\"species\"])\n",
        "\n",
        "# Identify column types\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "# 3) Preprocess: impute + encode + scale\n",
        "from sklearn.impute import SimpleImputer\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scale\", StandardScaler())\n",
        "        ]), num_cols),\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "        ]), cat_cols)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# 4) Train/val/test split (stratified)\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_trainval_prep = pre.fit_transform(X_trainval)\n",
        "X_test_prep = pre.transform(X_test)\n",
        "\n",
        "# 5) From-scratch k-NN (classifier)\n",
        "def pairwise_distances(A, B, metric=\"euclidean\"):\n",
        "    # A: [n_a, d], B: [n_b, d]\n",
        "    if metric == \"euclidean\":\n",
        "        # (a - b)^2 = a^2 + b^2 - 2ab\n",
        "        A2 = np.sum(A**2, axis=1, keepdims=True)       # [n_a, 1]\n",
        "        B2 = np.sum(B**2, axis=1)[None, :]             # [1, n_b]\n",
        "        AB = A @ B.T                                   # [n_a, n_b]\n",
        "        D2 = A2 + B2 - 2*AB\n",
        "        D2[D2 < 0] = 0\n",
        "        return np.sqrt(D2)\n",
        "    elif metric == \"manhattan\":\n",
        "        # |a - b| summed over dims\n",
        "        return np.sum(np.abs(A[:, None, :] - B[None, :, :]), axis=2)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported metric\")\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=5, metric=\"euclidean\", distance_weighted=False):\n",
        "        self.k = k\n",
        "        self.metric = metric\n",
        "        self.distance_weighted = distance_weighted\n",
        "        self.X_ = None\n",
        "        self.y_ = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_ = np.array(X, dtype=float)\n",
        "        self.y_ = np.array(y)\n",
        "        self.classes_ = np.unique(self.y_)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        D = pairwise_distances(X, self.X_, metric=self.metric)  # [n_test, n_train]\n",
        "        # argsort to find k nearest for each row\n",
        "        nn_idx = np.argpartition(D, kth=self.k-1, axis=1)[:, :self.k]  # [n_test, k]\n",
        "        preds = []\n",
        "        for i in range(X.shape[0]):\n",
        "            idx = nn_idx[i]\n",
        "            labels = self.y_[idx]\n",
        "            if self.distance_weighted:\n",
        "                # inverse distance weights (add small epsilon)\n",
        "                eps = 1e-8\n",
        "                weights = 1.0 / (D[i, idx] + eps)\n",
        "                # vote per class\n",
        "                votes = {c: 0.0 for c in self.classes_}\n",
        "                for lab, w in zip(labels, weights):\n",
        "                    votes[lab] += w\n",
        "                pred = max(votes.items(), key=lambda x: x[1])[0]\n",
        "            else:\n",
        "                # majority vote with deterministic tie-break\n",
        "                values, counts = np.unique(labels, return_counts=True)\n",
        "                max_count = np.max(counts)\n",
        "                tied = values[counts == max_count]\n",
        "                # tie-break by closest neighbor among tied classes\n",
        "                if len(tied) > 1:\n",
        "                    # pick class of the single closest neighbor among those tied\n",
        "                    closest_idx = idx[np.argmin(D[i, idx])]\n",
        "                    pred = self.y_[closest_idx]\n",
        "                    if pred not in tied:\n",
        "                        pred = np.random.choice(tied)  # fallback\n",
        "                else:\n",
        "                    pred = tied[0]\n",
        "            preds.append(pred)\n",
        "        return np.array(preds)\n",
        "\n",
        "# 6) Hyperparameter search for k\n",
        "def tune_k(X, y, ks=(1,3,5,7,9,11), metric=\"euclidean\", distance_weighted=False, folds=5):\n",
        "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "    results = []\n",
        "    for k in ks:\n",
        "        fold_scores = []\n",
        "        for tr_idx, va_idx in skf.split(X, y):\n",
        "            Xtr, Xva = X[tr_idx], X[va_idx]\n",
        "            ytr, yva = y[tr_idx], y[va_idx]\n",
        "            clf = KNNClassifier(k=k, metric=metric, distance_weighted=distance_weighted).fit(Xtr, ytr)\n",
        "            yhat = clf.predict(Xva)\n",
        "            fold_scores.append(accuracy_score(yva, yhat))\n",
        "        results.append((k, np.mean(fold_scores)))\n",
        "    best_k, best_acc = max(results, key=lambda t: t[1])\n",
        "    return best_k, results\n",
        "\n",
        "best_k, cv_table = tune_k(X_trainval_prep, y_trainval, ks=range(1,21,2), metric=\"euclidean\", distance_weighted=True)\n",
        "print(\"Best k:\", best_k)\n",
        "\n",
        "# 7) Train final model on all train+val, evaluate on held-out test\n",
        "clf = KNNClassifier(k=best_k, metric=\"euclidean\", distance_weighted=True).fit(X_trainval_prep, y_trainval)\n",
        "y_pred = clf.predict(X_test_prep)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 8) (Optional) Visualize decision regions via PCAâ†’2D\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "Z_train = pca.fit_transform(X_trainval_prep)\n",
        "Z_test = pca.transform(X_test_prep)\n",
        "\n",
        "clf2d = KNNClassifier(k=best_k, metric=\"euclidean\", distance_weighted=True).fit(Z_train, y_trainval)\n",
        "y_pred_2d = clf2d.predict(Z_test)\n",
        "print(\"PCA-2D kNN Test Accuracy:\", accuracy_score(y_test, y_pred_2d))\n"
      ],
      "metadata": {
        "id": "HCAL6IA4a0-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d08f0e2-c3c2-4140-ac3c-ab2c66b6d02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best k: 1\n",
            "Test Accuracy: 0.9855072463768116\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      0.97      0.98        30\n",
            "   Chinstrap       0.93      1.00      0.97        14\n",
            "      Gentoo       1.00      1.00      1.00        25\n",
            "\n",
            "    accuracy                           0.99        69\n",
            "   macro avg       0.98      0.99      0.98        69\n",
            "weighted avg       0.99      0.99      0.99        69\n",
            "\n",
            "PCA-2D kNN Test Accuracy: 0.8695652173913043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unrelated -Logistic Regression checking something\n",
        "# Logistic regression: loss, gradient, Hessian, Fisher metric, and a natural-gradient step\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def neg_log_lik(theta, X, y):\n",
        "    z = X @ theta\n",
        "    p = sigmoid(z)\n",
        "    # stabilize\n",
        "    eps = 1e-12\n",
        "    return -np.sum(y*np.log(p+eps) + (1-y)*np.log(1-p+eps))\n",
        "\n",
        "def grad(theta, X, y):\n",
        "    p = sigmoid(X @ theta)\n",
        "    return X.T @ (p - y)\n",
        "\n",
        "def hessian(theta, X):\n",
        "    p = sigmoid(X @ theta)\n",
        "    W = p*(1-p)                  # shape (n,)\n",
        "    return X.T * W @ X           # X.T @ diag(W) @ X, vectorized\n",
        "\n",
        "def fisher(theta, X):\n",
        "    # For logistic regression, expected Hessian = Fisher = X^T diag(p*(1-p)) X\n",
        "    return hessian(theta, X)\n",
        "\n",
        "def natural_grad_step(theta, X, y, lr=1.0, damping=1e-4):\n",
        "    g  = grad(theta, X, y)\n",
        "    F  = fisher(theta, X)\n",
        "    # damped solve (Tikhonov) for numerical stability\n",
        "    step = np.linalg.solve(F + damping*np.eye(F.shape[0]), g)\n",
        "    return theta - lr * step\n",
        "\n",
        "# Diagnostics: SVD & principal directions of curvature\n",
        "def curvature_decomp(theta, X):\n",
        "    # SVD of data operator\n",
        "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
        "    # Hessian/Fisher at theta\n",
        "    H = hessian(theta, X)\n",
        "    evals, evecs = np.linalg.eigh(H)  # symmetric PSD\n",
        "    return (U, S, Vt.T), (evals, evecs)\n",
        "\n",
        "# Example usage (toy):\n",
        "# X: (n_samples x d_features), y in {0,1}\n",
        "# theta0 = np.zeros(d_features)\n",
        "# for _ in range(10):\n",
        "#     theta0 = natural_grad_step(theta0, X, y, lr=1.0, damping=1e-3)\n",
        "# (U,S,V), (lam, Q) = curvature_decomp(theta0, X)\n"
      ],
      "metadata": {
        "id": "T4YDQP2BxiQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gX_xvzKBa1C_"
      }
    }
  ]
}